#  Ask Nietzsche â€“ AI Philosophy Chatbot'

**Ask Nietzsche** is an AI-powered chatbot that answers your deepest existential, moral, and philosophical questions in the style of **Friedrich Nietzsche**, blending historical insight with modern AI. Built using Streamlit and OpenAI's language models, this app aims to make Nietzsche's philosophy accessible, conversational, and oddly motivating.

## âœ¨ Features

- ğŸ—£ï¸ Ask any life/philosophy-related question
- ğŸ“œ Choose between "Classic Nietzsche" (authentic quotes) or "Modern Nietzsche" (AI-generated responses)
- ğŸ§  Smart prompt engine trained on Nietzsche's core ideas (e.g., Will to Power, Eternal Recurrence)
- ğŸ“š Quote retrieval using embeddings (vector search coming soon)
- ğŸ•°ï¸ Long-term memory agent (in progress)
- ğŸ”§ Modular agent-based architecture MVP Phase

## ğŸ“¦ Tech Stack

- **Frontend**: [Streamlit](https://streamlit.io/)
- **Backend**: Python, OpenAI API
- **AI Agent Framework**: LangChain (planned)
- **Data**: Nietzscheâ€™s public domain texts + curated quote DB
- **Optional**: FAISS for vector-based quote retrieval (for smart responses)

## ğŸš€ Getting Started

### 1. Clone the Repository

    ```bash
        git clone https://github.com/vij-sameerb5/ask-nietzsche.git
        cd ask-nietzsche

    ```bash
      pip install -r requirements.txt

      ```bash
      streamlit run ask_nietzsche.py

ğŸš€ Whatâ€™s Happened So Far
MVP Scaffold

Built a Streamlit app (ask_nietzsche.py) with Classic (quote lookup) and Modern (LLMâ€‘generated) modes.

Created modular agents: PromptDesigner, ContextAgent, ResponseGenerator, CriticAgent, MemoryAgent.

Modern Mode Integration

Hooked up a HuggingÂ Face LlamaÂ 2 Chatâ€‘HF model via transformers.from_pretrained(...).

Installed SentencePiece and authenticated with your HF token.

Gating & Download Errors

Hit 403 errors because LlamaÂ 2 Chatâ€‘HF is gatedâ€”needed to request access on HuggingÂ Face.

Tried a direct download script (snapshot_download) but still lacked permissions.

Performance Tuning

Tried bitsandbytes 8â€‘bit quant on MPS (failedâ€”unsupported on Apple GPUs).

Reviewed three paths to get fast, free inference.

ğŸ”§ What Weâ€™re Doing Right Now
You chose OptionÂ C: run a quantized GGML binary via llama-cpp-python:

Converted the PyTorch LlamaÂ 2 Chatâ€‘HF weights into a 4â€‘bit GGML file (.ggml.q4_0.bin).

Installed the llama-cpp-python binding.

Rewired ResponseGenerator to load that local GGML file and generate on CPU in ~1â€“3Â s per reply.

Your Streamlit app now uses this pureâ€‘local, zeroâ€‘APIâ€‘fee backend.

ğŸ”„ The Three Options We Discussed

Option	How It Works	Speed	Cost	Pros / Cons
A) GPTâ€‘3.5â€‘Turbo	OpenAI API calls for each reply	~0.5â€“1Â s	$0.002/1KÂ tokens	Fastest iteration; payâ€‘perâ€‘use fees
B) Smaller HF Model	Use a lighter public model (e.g. MistralÂ 7B) on MPS	~3â€“5Â s	$0 (once cached)	No gating; simpler transformers code
C) GGML Quant (you chose)	Load a Q4_0 binary via llama-cpp-python	~1â€“3Â s	$0	Fully local; no GPU required
ğŸ Next Steps
Verify that â€œModern Nietzscheâ€ now returns a true continuation in Nietzscheâ€™s tone.

Tweak sampling (temperature, top_p, max_tokens) in your llama_cpp call to get the voice and length you like.

Polish your PromptDesigner or add followâ€‘up prompts to enrich the dialogue.

Expand your quotes.json and tests so Classic mode covers more topics.

Deploy (Streamlit Cloud or HuggingÂ Face Spaces) and share your Nietzsche chat with the world!

Let me know how the CPUâ€‘quantized model feels, and we can adjust any final parameters together.







